## This program demonstrates how to train and evaluate machine learning models on two different datasets: MNIST (handwritten digits) and CIFAR-10 (image classification). Here's what the code does in detail:

```python
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sn

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Display dataset details
print(f"Number of test samples: {len(x_test)}")
print(f"Number of training samples: {len(x_train)}")
print(f"Shape of a single image: {x_train[0].shape}")
print(f"Sample label (y_train[7]): {y_train[7]}")

# Visualize a sample image
plt.matshow(x_train[1000])
plt.show()

# Normalize the data
x_train = x_train / 255
x_test = x_test / 255

# Flatten the data
x_train_flattened = x_train.reshape(len(x_train), 28 * 28)
x_test_flattened = x_test.reshape(len(x_test), 28 * 28)

# Print shapes for verification
print(f"Flattened training data shape: {x_train_flattened.shape}")

# Define a simple neural network model
model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train_flattened, y_train, epochs=5)

# Evaluate the model
model.evaluate(x_test_flattened, y_test)

# Redefine the model with additional layers
model = keras.Sequential([
    keras.layers.Dense(100, input_shape=(784,), activation='relu'),
    keras.layers.Dense(50, activation='sigmoid'),
    keras.layers.Dense(10, activation='sigmoid')
])

# Compile and train the updated model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train_flattened, y_train, epochs=5)

# Predictions
y_predicted = model.predict(x_test_flattened)
print(f"Prediction for first sample: {np.argmax(y_predicted[0])}")

# Visualize a test image
plt.matshow(x_test[4])
plt.show()

# Predicted label for the 4th test sample
print(f"Predicted label for 4th test sample: {np.argmax(y_predicted[4])}")

# Convert predictions to class labels
y_predicted_labels = [np.argmax(i) for i in y_predicted]

# Confusion Matrix
cm = tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')
plt.show()

# Load the CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()

# Reshape labels
y_train = y_train.flatten()
y_test = y_test.flatten()

# Class names for CIFAR-10
classes = ["airplane", "automobile", "bird", "cat", "deer", 
           "dog", "frog", "horse", "ship", "truck"]

# Function to plot a sample image
def plot_sample(x, y, index):
    plt.figure(figsize=(15, 2))
    plt.imshow(x[index])
    plt.xlabel(classes[y[index]])

# Plot a sample image from CIFAR-10
plot_sample(x_train, y_train, 30)
```

### Explanation of Formatting
1. **Comments**: Added descriptive comments to explain each step.
2. **Logical Sections**: Split the MNIST and CIFAR-10 sections for better modularity.
3. **Improved Visualization**: Enhanced visualization with clear labels.
4. **Functions**: Used a function (`plot_sample`) for reusability.

This code is now cleaner and easier to read. Let me know if you'd like further improvements!